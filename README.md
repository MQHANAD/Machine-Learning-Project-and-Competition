# Machine-Learning-Project-and-Competition

Course term project and Kaggle competition work for **ICS 485 – Machine Learning (Term 251)**.

This repository contains:
- Course-required notebooks (Part A & Part B)
- **Three dedicated Kaggle notebooks** for experimentation and final submission

---

## Repository Structure

- `PartA.ipynb`  
  Full solution for Part A: EDA, preprocessing, feature engineering, multiple models, and evaluation.

- `PartB.ipynb`  
  Active Learning implementation using:
  - Least Confidence Sampling
  - Entropy Sampling  
  (Classifier: Logistic Regression)

- `kaggle.ipynb`  
  Kaggle notebook #1 — baseline model and fast experimentation.

- `kaggle_v2.ipynb`  
  Kaggle notebook #2 — improved feature engineering and model tuning.

- `kaggle_final.ipynb`  
  Kaggle notebook #3 — final ensemble, threshold optimization, and submission generation.

- `dataset-train-vf.csv`  
  Training dataset (includes labels).

- `dataset-test-vf.csv`  
  Test dataset (no labels).

- `Submission.csv`  
  Kaggle submission template.

- `Submission_final_last_try.csv`  
  Final submission file generated by the last Kaggle notebook.

---

## Problem Statement

Binary classification task: predict whether each sample represents a **square** or a **circle**.

- Classes: `square`, `circle`
- Evaluation metric: **Macro F1-score**
- Focus: correct handling of class imbalance and minority class detection

---

## Methodology Overview

### 1. Exploratory Data Analysis (EDA)
- Class imbalance inspection
- Missing value analysis
- Feature distribution and correlation analysis

### 2. Preprocessing
- Encode target labels (`square → 0`, `circle → 1`)
- Encode categorical features (e.g., `f11`) using `LabelEncoder`
- Preserve missing values for tree-based models

### 3. Feature Engineering
- Ratio-based features (e.g., `f1 / f6`, `f5 / f6`, `f5 / f9`)
- Log-space transformations and log-difference features
- Missing-value indicator features
- Lightweight interaction features

### 4. Class Imbalance Handling
- Use `scale_pos_weight` in LightGBM to address minority class imbalance

### 5. Modeling & Validation
- Stratified K-Fold Cross Validation
- LightGBM ensemble:
  - GBDT
  - DART
- Multiple random seeds for variance reduction
- Early stopping for generalization

### 6. Threshold Optimization
- Optimize decision threshold on out-of-fold predictions
- Maximize **Macro F1-score**
- Avoid fixed threshold assumptions (e.g., 0.5)

---

## Kaggle Submissions

- Use `Submission.csv` as the submission template.
- Final predictions are written to `Submission_final_last_try.csv`.

> Note: Public leaderboard scores may differ from cross-validation results due to class imbalance and threshold sensitivity.

---

## Notes

- This repository is intended for **academic and learning purposes only**.
- The Kaggle competition associated with this project is **course-specific**.

---

## Author

**Muhannad (MQHANAD)**  
GitHub: https://github.com/MQHANAD

